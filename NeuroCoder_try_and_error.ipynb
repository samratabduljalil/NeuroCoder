{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samra\\AppData\\Local\\Temp\\ipykernel_5116\\121158271.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  research_llm = Ollama(model=\"deepseek-r1\")  # Use DeepSeek R1 for research tasks\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Agent\ntools\n  Value error, Invalid tool type: <class 'langchain_community.utilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper'>. Tool must be an instance of BaseTool or an object with 'name', 'func', and 'description' attributes. [type=value_error, input_value=[DuckDuckGoSearchAPIWrapp...='auto', source='text')], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m custom_tools \u001b[38;5;241m=\u001b[39m CustomTools()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Define Agents\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m research_agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSenior Research Analyst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgoal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFind and analyze the most impactful academic papers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackstory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mYou\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mre a renowned research analyst with a knack for finding \u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;43m    cutting-edge papers in any field. You have access to all major academic \u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;43m    databases and can quickly assess paper quality.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_delegation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msearch_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use DuckDuckGo search tool\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresearch_llm\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use DeepSeek R1 for research\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m code_agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[0;32m     39\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSenior Software Engineer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     40\u001b[0m     goal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerate high-quality code implementations from research methodologies\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     llm\u001b[38;5;241m=\u001b[39mcode_llm  \u001b[38;5;66;03m# Use DeepSeek Coder for coding\u001b[39;00m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Define Tasks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Agent\ntools\n  Value error, Invalid tool type: <class 'langchain_community.utilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper'>. Tool must be an instance of BaseTool or an object with 'name', 'func', and 'description' attributes. [type=value_error, input_value=[DuckDuckGoSearchAPIWrapp...='auto', source='text')], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain.tools import tool\n",
    "#from langchain_community.tools import DuckDuckGoSearchAPIWrapper\n",
    "#from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize local LLMs using Ollama\n",
    "research_llm = Ollama(model=\"deepseek-r1\")  # Use DeepSeek R1 for research tasks\n",
    "code_llm = Ollama(model=\"deepseek-coder\")  # Use DeepSeek Coder for coding tasks\n",
    "\n",
    "# Custom Tools (if needed)\n",
    "class CustomTools:\n",
    "    @tool(\"Save to File\")\n",
    "    def save_to_file(content: str, filename: str):\n",
    "        \"\"\"Saves content to a file.\"\"\"\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.write(content)\n",
    "        return f\"Content saved to {filename}\"\n",
    "\n",
    "# Initialize tools\n",
    "search_tool = GoogleSerperAPIWrapper()  # Free search API\n",
    "custom_tools = CustomTools()\n",
    "\n",
    "# Define Agents\n",
    "research_agent = Agent(\n",
    "    role='Senior Research Analyst',\n",
    "    goal='Find and analyze the most impactful academic papers',\n",
    "    backstory=\"\"\"You're a renowned research analyst with a knack for finding \n",
    "    cutting-edge papers in any field. You have access to all major academic \n",
    "    databases and can quickly assess paper quality.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool.run],  # Use DuckDuckGo search tool\n",
    "    llm=research_llm  # Use DeepSeek R1 for research\n",
    ")\n",
    "\n",
    "code_agent = Agent(\n",
    "    role='Senior Software Engineer',\n",
    "    goal='Generate high-quality code implementations from research methodologies',\n",
    "    backstory=\"\"\"You're a principal engineer at a top AI lab who specializes \n",
    "    in translating research papers into working implementations.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[custom_tools.save_to_file],\n",
    "    llm=code_llm  # Use DeepSeek Coder for coding\n",
    ")\n",
    "\n",
    "# Define Tasks\n",
    "research_task = Task(\n",
    "    description=\"\"\"Find the best paper on {topic}. Extract its methodology section \n",
    "    and provide a detailed explanation of the approach.\"\"\",\n",
    "    expected_output=\"A detailed methodology section from the best paper on the topic.\",\n",
    "    agent=research_agent,\n",
    "    output_file='methodology.txt'\n",
    ")\n",
    "\n",
    "coding_task = Task(\n",
    "    description=\"\"\"Take the methodology from the research agent and generate Python code \n",
    "    that implements the described approach. Include test cases and documentation.\"\"\",\n",
    "    expected_output=\"Complete Python implementation with example usage.\",\n",
    "    agent=code_agent,\n",
    "    output_file='implementation.py'\n",
    ")\n",
    "\n",
    "# Create Crew\n",
    "implementation_crew = Crew(\n",
    "    agents=[research_agent, code_agent],\n",
    "    tasks=[research_task, coding_task],\n",
    "    verbose=2,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter research topic: \")\n",
    "    result = implementation_crew.kickoff(inputs={'topic': topic})\n",
    "    print(\"\\n\\n########################\")\n",
    "    print(\"## Workflow Complete ##\")\n",
    "    print(\"########################\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Ollama\nlitellm_provider\n  Extra inputs are not permitted [type=extra_forbidden, input_value='ollama', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[0;32m     18\u001b[0m OLLAMA_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:12345\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m research_llm \u001b[38;5;241m=\u001b[39m \u001b[43mOllama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-r1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOLLAMA_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicitly define the provider\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize search tool using Google Serper API\u001b[39;00m\n\u001b[0;32m     27\u001b[0m search_tool \u001b[38;5;241m=\u001b[39m GoogleSerperAPIWrapper(serper_api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Ollama\nlitellm_provider\n  Extra inputs are not permitted [type=extra_forbidden, input_value='ollama', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "#import spacy\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "#from langchain_community.llms import Ollama\n",
    "#from langchain.llms import Ollama\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Load spaCy model for NLP\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize local LLM for research tasks\n",
    "#OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "#research_llm = Ollama(model=\"deepseek-r1:latest\",base_url=OLLAMA_URL)  # Use DeepSeek R1 for research\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "research_llm = Ollama(\n",
    "    model=\"deepseek-r1\",\n",
    "    base_url=OLLAMA_URL,\n",
    "    litellm_provider=\"ollama\"  # Explicitly define the provider\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize search tool using Google Serper API\n",
    "search_tool = GoogleSerperAPIWrapper(serper_api_key=\"8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\")\n",
    "search_tool_wrapped = Tool(\n",
    "    name=\"Google Serper Search\",\n",
    "    func=search_tool.run,\n",
    "    description=\"Searches Google for relevant academic papers\"\n",
    ")\n",
    "# Define Research Agent\n",
    "research_agent = Agent(\n",
    "    role='Senior Research Analyst',\n",
    "    goal='Find and analyze the most impactful academic papers',\n",
    "    backstory=\"\"\"You're a renowned research analyst with a knack for finding \n",
    "    cutting-edge papers in any field. You have access to all major academic \n",
    "    databases and can quickly assess paper quality.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool_wrapped],\n",
    "    llm=research_llm  # Use DeepSeek R1 for research\n",
    ")\n",
    "\n",
    "# Define Research Task\n",
    "research_task = Task(\n",
    "    description=\"Find the best paper on {topic} using Google Serper API.\",\n",
    "    expected_output=\"A list of relevant research papers on the topic.\",\n",
    "    agent=research_agent,\n",
    "    output_file='search_results.txt'\n",
    ")\n",
    "\n",
    "# Create Crew\n",
    "search_crew = Crew(\n",
    "    agents=[research_agent],\n",
    "    tasks=[research_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter research topic: \")\n",
    "    result = search_crew.kickoff(inputs={'topic': topic})\n",
    "    print(\"\\n\\n########################\")\n",
    "    print(\"## Search Complete ##\")\n",
    "    print(\"########################\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain machine learning in simple terms. Hmm, where do I start? Well, maybe first think about what machine learning is all about. From what I understand, it's a way for computers to learn from data without being explicitly programmed. But how exactly does that work?\n",
      "\n",
      "Maybe I should break it down into simpler concepts. Like, when you teach a child to recognize objects, they see many examples and then can identify them. Is machine learning similar? Instead of a teacher, the computer learns by analyzing patterns in data.\n",
      "\n",
      "What are some common applications? Well, recommendation systems on Netflix or Spotify probably use ML to suggest shows or songs based on your past choices. Maybe facial recognition is another example; it's used in security or apps like Instagram's filters.\n",
      "\n",
      "So, how does the process actually happen? I think it involves feeding a lot of data into an algorithm, which then finds patterns within that data. Once the model learns from this data, it can make predictions or decisions without being reprogrammed for each new task.\n",
      "\n",
      "Wait, but what about overfitting? Isn't there a risk where the model gets too specific to the training data and doesn't perform well on new data? Maybe I should mention that briefly, but not get too technical since the user asked for simple terms.\n",
      "\n",
      "Also, comparing it to something familiar might help. Like teaching a child to recognize apples by showing them many examples—similarly, ML programs are taught using data instead of explicit rules each time.\n",
      "\n",
      "I should keep the language straightforward and avoid jargon as much as possible. Maybe use analogies people can relate to, like teaching without being told every rule again.\n",
      "\n",
      "Putting it all together: machine learning is about teaching computers to recognize patterns in data so they can make decisions or predictions based on that information. It's like showing a child lots of examples until they get it right.\n",
      "\n",
      "I think that covers the basics. Now I just need to structure it in simple, clear terms without making it too complicated.\n",
      "</think>\n",
      "\n",
      "Machine learning is a way for computers to learn and improve from experience without being explicitly programmed. It’s like teaching a child to recognize an apple by showing them many examples until they understand what makes something an apple. Instead of giving the computer rules each time, it uses data patterns to make decisions or predictions, much like how a child learns through observation and practice.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "llm = Ollama(model=\"deepseek-r1\", base_url=OLLAMA_URL)\n",
    "\n",
    "response = llm.invoke(\"Explain machine learning in simple terms.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter research topic:  cwt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Research Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mFind the best paper on cwt using Google Serper API.\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[91m Error during LLM call: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\nParams: {'model': 'deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     47\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter research topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_crew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m########################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Search Complete ##\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:558\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    555\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[1;32m--> 558\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[0;32m    560\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:665\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[0;32m    664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:767\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    764\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    766\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 767\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agent.py:259\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agent.py:248\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    245\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:112\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_unknown_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    105\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:160\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_context_length_exceeded(e):\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_context_length()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_rpm_limit()\n\u001b[1;32m--> 140\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_llm_response(answer)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:210\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    214\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:201\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\llm.py:252\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    249\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# --- 2) Make the completion call\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    253\u001b[0m response_message \u001b[38;5;241m=\u001b[39m cast(Choices, cast(ModelResponse, response)\u001b[38;5;241m.\u001b[39mchoices)[\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    255\u001b[0m ]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    256\u001b[0m text_response \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\utils.py:1100\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1097\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1098\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1099\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\utils.py:978\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    979\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\main.py:2981\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   2979\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 2981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   2982\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2983\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   2984\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   2985\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2986\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   2987\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\main.py:943\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[0;32m    942\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 943\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    951\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[0;32m    953\u001b[0m ):\n\u001b[0;32m    954\u001b[0m     headers\u001b[38;5;241m.\u001b[39mupdate(provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[1;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    331\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[0;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[0;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         ),\n\u001b[0;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[0;32m    346\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\nParams: {'model': 'deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Initialize Ollama for LLM processing\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "research_llm = Ollama(model=\"deepseek-r1\", base_url=OLLAMA_URL)  # Ensure the model exists\n",
    "\n",
    "# Initialize Google Serper API tool\n",
    "search_tool = GoogleSerperAPIWrapper(serper_api_key=\"8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\")\n",
    "search_tool_wrapped = Tool(\n",
    "    name=\"Google Serper Search\",\n",
    "    func=search_tool.run,\n",
    "    description=\"Searches Google for relevant academic papers\"\n",
    ")\n",
    "\n",
    "# Define Research Agent with LLM\n",
    "research_agent = Agent(\n",
    "    role=\"Senior Research Analyst\",\n",
    "    goal=\"Find and analyze the most impactful academic papers\",\n",
    "    backstory=\"You're a research expert who specializes in finding top papers in any field.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool_wrapped],\n",
    "    llm=research_llm  # ✅ Ensure Ollama is used properly\n",
    ")\n",
    "\n",
    "# Define Research Task\n",
    "research_task = Task(\n",
    "    description=\"Find the best paper on {topic} using Google Serper API.\",\n",
    "    expected_output=\"A list of relevant research papers on the topic.\",\n",
    "    agent=research_agent,\n",
    "    output_file=\"search_results.txt\"\n",
    ")\n",
    "\n",
    "# Create Crew\n",
    "search_crew = Crew(\n",
    "    agents=[research_agent],\n",
    "    tasks=[research_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter research topic: \")\n",
    "    result = search_crew.kickoff(inputs={\"topic\": topic})\n",
    "\n",
    "    print(\"\\n\\n########################\")\n",
    "    print(\"## Search Complete ##\")\n",
    "    print(\"########################\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "It seems like you might be testing or checking something. How can I assist you? If you need information, a tool, or have a question, feel free to let me know! 😊\n"
     ]
    }
   ],
   "source": [
    "response = research_llm.invoke(\"Test query\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter research topic:  cow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Research Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mFind the best paper on cow using Google Serper API.\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[91m Error during LLM call: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\nParams: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     50\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter research topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_crew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m########################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Search Complete ##\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:558\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    555\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[1;32m--> 558\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[0;32m    560\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:665\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[0;32m    664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:767\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    764\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    766\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 767\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agent.py:259\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agent.py:248\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    245\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:112\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_unknown_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    105\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:160\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_context_length_exceeded(e):\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_context_length()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_rpm_limit()\n\u001b[1;32m--> 140\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_llm_response(answer)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:210\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    214\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:201\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\llm.py:252\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    249\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# --- 2) Make the completion call\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    253\u001b[0m response_message \u001b[38;5;241m=\u001b[39m cast(Choices, cast(ModelResponse, response)\u001b[38;5;241m.\u001b[39mchoices)[\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    255\u001b[0m ]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    256\u001b[0m text_response \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\utils.py:1100\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1097\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1098\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1099\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\utils.py:978\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    979\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\main.py:2981\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   2979\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 2981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   2982\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2983\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   2984\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   2985\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2986\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   2987\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\main.py:943\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[0;32m    942\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 943\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    951\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[0;32m    953\u001b[0m ):\n\u001b[0;32m    954\u001b[0m     headers\u001b[38;5;241m.\u001b[39mupdate(provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[1;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    331\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[0;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[0;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         ),\n\u001b[0;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[0;32m    346\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\nParams: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Initialize Ollama for LLM processing\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "research_llm = Ollama(\n",
    "    model=\"ollama/deepseek-r1\",  # Explicitly specify the provider\n",
    "    base_url=OLLAMA_URL\n",
    ")\n",
    "\n",
    "# Initialize Google Serper API tool\n",
    "search_tool = GoogleSerperAPIWrapper(serper_api_key=\"8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\")\n",
    "search_tool_wrapped = Tool(\n",
    "    name=\"Google Serper Search\",\n",
    "    func=search_tool.run,\n",
    "    description=\"Searches Google for relevant academic papers\"\n",
    ")\n",
    "\n",
    "# Define Research Agent with LLM\n",
    "research_agent = Agent(\n",
    "    role=\"Senior Research Analyst\",\n",
    "    goal=\"Find and analyze the most impactful academic papers\",\n",
    "    backstory=\"You're a research expert who specializes in finding top papers in any field.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool_wrapped],\n",
    "    llm=research_llm  # ✅ Ensure Ollama is used properly\n",
    ")\n",
    "\n",
    "# Define Research Task\n",
    "research_task = Task(\n",
    "    description=\"Find the best paper on {topic} using Google Serper API.\",\n",
    "    expected_output=\"A list of relevant research papers on the topic.\",\n",
    "    agent=research_agent,\n",
    "    output_file=\"search_results.txt\"\n",
    ")\n",
    "\n",
    "# Create Crew\n",
    "search_crew = Crew(\n",
    "    agents=[research_agent],\n",
    "    tasks=[research_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter research topic: \")\n",
    "    result = search_crew.kickoff(inputs={\"topic\": topic})\n",
    "\n",
    "    print(\"\\n\\n########################\")\n",
    "    print(\"## Search Complete ##\")\n",
    "    print(\"########################\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter research topic:  cow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Research Analyst\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mFind the best paper on cow using Google Serper API.\u001b[00m\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\u001b[91m Error during LLM call: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[00m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\nParams: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     50\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter research topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_crew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m########################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Search Complete ##\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:558\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    555\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[1;32m--> 558\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[0;32m    560\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:665\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[0;32m    664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\crew.py:767\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    764\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    766\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 767\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agent.py:259\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agent.py:248\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    245\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:112\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_unknown_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    105\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:160\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_context_length_exceeded(e):\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_context_length()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_rpm_limit()\n\u001b[1;32m--> 140\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_llm_response(answer)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:210\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    214\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:201\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\crewai\\llm.py:252\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    249\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# --- 2) Make the completion call\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    253\u001b[0m response_message \u001b[38;5;241m=\u001b[39m cast(Choices, cast(ModelResponse, response)\u001b[38;5;241m.\u001b[39mchoices)[\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    255\u001b[0m ]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    256\u001b[0m text_response \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\utils.py:1100\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1097\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1098\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1099\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\utils.py:978\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    979\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\main.py:2981\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   2979\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 2981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   2982\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2983\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   2984\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   2985\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2986\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   2987\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\main.py:943\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[0;32m    942\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 943\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    951\u001b[0m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m custom_llm_provider\n\u001b[0;32m    953\u001b[0m ):\n\u001b[0;32m    954\u001b[0m     headers\u001b[38;5;241m.\u001b[39mupdate(provider_specific_header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[1;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NeuroCoder\\lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[1;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[0;32m    331\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[0;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[0;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[0;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         ),\n\u001b[0;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[0;32m    346\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mOllama\u001b[0m\nParams: {'model': 'ollama/deepseek-r1', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Initialize Ollama for LLM processing\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "research_llm = Ollama(\n",
    "    model=\"ollama/deepseek-r1\",  # Explicitly specify the provider\n",
    "    base_url=OLLAMA_URL\n",
    ")\n",
    "\n",
    "# Initialize Google Serper API tool\n",
    "search_tool = GoogleSerperAPIWrapper(serper_api_key=\"8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\")\n",
    "search_tool_wrapped = Tool(\n",
    "    name=\"Google Serper Search\",\n",
    "    func=search_tool.run,\n",
    "    description=\"Searches Google for relevant academic papers\"\n",
    ")\n",
    "\n",
    "# Define Research Agent with LLM\n",
    "research_agent = Agent(\n",
    "    role=\"Senior Research Analyst\",\n",
    "    goal=\"Find and analyze the most impactful academic papers\",\n",
    "    backstory=\"You're a research expert who specializes in finding top papers in any field.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool_wrapped],\n",
    "    llm=research_llm  # ✅ Ensure Ollama is used properly\n",
    ")\n",
    "\n",
    "# Define Research Task\n",
    "research_task = Task(\n",
    "    description=\"Find the best paper on {topic} using Google Serper API.\",\n",
    "    expected_output=\"A list of relevant research papers on the topic.\",\n",
    "    agent=research_agent,\n",
    "    output_file=\"search_results.txt\"\n",
    ")\n",
    "\n",
    "# Create Crew\n",
    "search_crew = Crew(\n",
    "    agents=[research_agent],\n",
    "    tasks=[research_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter research topic: \")\n",
    "    result = search_crew.kickoff(inputs={\"topic\": topic})\n",
    "\n",
    "    print(\"\\n\\n########################\")\n",
    "    print(\"## Search Complete ##\")\n",
    "    print(\"########################\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomOllamaLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tool\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize Custom Ollama LLM\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m research_llm \u001b[38;5;241m=\u001b[39m \u001b[43mCustomOllamaLLM\u001b[49m(base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:12345\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-r1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize Google Serper API tool\u001b[39;00m\n\u001b[0;32m      9\u001b[0m search_tool \u001b[38;5;241m=\u001b[39m GoogleSerperAPIWrapper(serper_api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomOllamaLLM' is not defined"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Initialize Custom Ollama LLM\n",
    "research_llm = CustomOllamaLLM(base_url=\"http://127.0.0.1:12345\", model=\"deepseek-r1\")\n",
    "\n",
    "# Initialize Google Serper API tool\n",
    "search_tool = GoogleSerperAPIWrapper(serper_api_key=\"8a8810d7af80d6ef588cdd12b9e913ca20baa4c0\")\n",
    "search_tool_wrapped = Tool(\n",
    "    name=\"Google Serper Search\",\n",
    "    func=search_tool.run,\n",
    "    description=\"Searches Google for relevant academic papers\"\n",
    ")\n",
    "\n",
    "# Define Research Agent with Custom LLM\n",
    "research_agent = Agent(\n",
    "    role=\"Senior Research Analyst\",\n",
    "    goal=\"Find and analyze the most impactful academic papers\",\n",
    "    backstory=\"You're a research expert who specializes in finding top papers in any field.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_tool_wrapped],\n",
    "    llm=research_llm  # Use the custom Ollama LLM\n",
    ")\n",
    "\n",
    "# Define Research Task\n",
    "research_task = Task(\n",
    "    description=\"Find the best paper on {topic} using Google Serper API.\",\n",
    "    expected_output=\"A list of relevant research papers on the topic.\",\n",
    "    agent=research_agent,\n",
    "    output_file=\"search_results.txt\"\n",
    ")\n",
    "\n",
    "# Create Crew\n",
    "search_crew = Crew(\n",
    "    agents=[research_agent],\n",
    "    tasks=[research_task],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter research topic: \")\n",
    "    result = search_crew.kickoff(inputs={\"topic\": topic})\n",
    "\n",
    "    print(\"\\n\\n########################\")\n",
    "    print(\"## Search Complete ##\")\n",
    "    print(\"########################\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
